{"pages":[{"title":"Jupyter Notebook and Jupyter Lab","text":"Project Jupyter Project Jupyter has developped the Jupyter Notebook, an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. JupyterLab is a web-based user interface for Project Jupyter. It enables you to work with Jupyter notebooks, editors, terminals, in a web-based IDE. Jupyter notebook example You can visualize notebooks in the following site https://nbviewer.jupyter.org/ . The other solution is to install jupyter notebooks. It will then run in your web browser. This solution is better as it will allow you to modify and run the code. Here is an static example of a jupyter notebook : In [2]: import altair as alt from vega_datasets import data In [2]: # Uncomment/run this line to enable Altair in the classic notebook (not JupyterLab) alt . renderers . enable ( 'notebook' ) In [6]: iris = data . iris () iris . head () Out[6]: .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; } petalLength petalWidth sepalLength sepalWidth species 0 1.4 0.2 5.1 3.5 setosa 1 1.4 0.2 4.9 3.0 setosa 2 1.3 0.2 4.7 3.2 setosa 3 1.5 0.2 4.6 3.1 setosa 4 1.4 0.2 5.0 3.6 setosa In [7]: alt . Chart ( iris ) . mark_square () . encode ( x = 'petalLength' , y = 'petalWidth' , color = 'species' ) Out[7]: First we'll create an interval selection using the selection_interval() function: In [8]: brush = alt . selection_interval () # selection of type \"interval\" In [11]: alt . Chart ( iris ) . mark_square () . encode ( x = 'petalLength' , y = 'petalWidth' , color = alt . condition ( brush , 'species:N' , alt . value ( 'lightgray' )) ) . properties ( selection = brush ) Out[11]: In [17]: brush = alt . selection_interval ( encodings = [ 'x' ]) # selection of type \"interval\" chart = alt . Chart ( iris ) . mark_square () . encode ( y = 'petalWidth' , color = alt . condition ( brush , 'species:N' , alt . value ( 'lightgray' )) ) . properties ( selection = brush ) In [18]: chart . encode ( x = 'petalLength' ) | chart . encode ( x = 'sepalLength' ) Out[18]:","tags":"python","url":"https://tinmard.github.io/jupyter-notebook-and-jupyter-lab.html"},{"title":"Stimic - Analysis - Artifact Carac","text":"Caracterization of the stimulation artefact In [1]: % matplotlib inline import mne import pandas as pd import numpy as np import matplotlib.pyplot as plt import sys import seaborn as sns sys . path . append ( '../stimic_main/' ) import utils_stimic import utils_eeg_cleaning from stim_analysis_fun import * sns . set () sns . set_context ( 'paper' ) plt . rcParams [ 'figure.figsize' ] = ( 14 , 14 ) Set the path of the epoch and stim event list : In [2]: stim_epoch_path = r 'C: \\\\ Users \\\\ deudon \\\\ Desktop \\\\ Stimic \\\\ _Data \\\\ STIMS \\\\ P53_CD25 \\\\ P53_CD25_stim_mne_epoch-resync-epo.fif' stim_spreadsheet_path = r 'C:\\Users\\deudon\\Desktop\\Stimic\\_Data\\STIMS\\P53_CD25\\p53_CD25_stimulations_all.xlsx' Read mne epoch structure : In [3]: stim_epoch = mne . read_epochs ( stim_epoch_path ) print ( stim_epoch ) Reading C:\\\\Users\\\\deudon\\\\Desktop\\\\Stimic\\\\_Data\\\\STIMS\\\\P53_CD25\\\\P53_CD25_stim_mne_epoch-resync-epo.fif ... Isotrak not found Found the data of interest: t = -5000.00 ... 24998.05 ms 0 CTF compensation matrices available 182 matching events found 182 matching events found 0 projection items activated <EpochsFIF | n_events : 182 (all good), tmin : -5.0 (s), tmax : 24.998046875 (s), baseline : None, ~2.44 GB, data loaded> Load stim spreadsheet : In [4]: df = pd . read_excel ( stim_spreadsheet_path ) df . head () Out[4]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID file type time channelind channelname freq intensity duration effect channelname_bipolar time-micro channelname-micro channelind-micro filepath-micro 0 P53_CD25 CD25_stim1_clean_clean_raw Stim 1 Hz 174.4736 96 EEG GC1 1 1.5 10 RAS EEG GC1-EEG GC2 217.1523 [] [] C:\\Users\\deudon\\Desktop\\Stimic\\_Data\\STIMS\\P53... 1 P53_CD25 CD25_stim1_clean_clean_raw Stim 1 Hz 174.4834 97 EEG GC2 1 1.5 10 RAS EEG GC1-EEG GC2 217.1621 [] [] C:\\Users\\deudon\\Desktop\\Stimic\\_Data\\STIMS\\P53... 2 P53_CD25 CD25_stim1_clean_clean_raw Stim 1 Hz 234.5498 97 EEG GC2 1 2.0 10 RAS EEG GC2-EEG GC3 277.2285 [] [] C:\\Users\\deudon\\Desktop\\Stimic\\_Data\\STIMS\\P53... 3 P53_CD25 CD25_stim1_clean_clean_raw Stim 1 Hz 234.5566 98 EEG GC3 1 2.0 10 RAS EEG GC2-EEG GC3 277.2353 [] [] C:\\Users\\deudon\\Desktop\\Stimic\\_Data\\STIMS\\P53... 4 P53_CD25 CD25_stim1_clean_clean_raw Stim 1 Hz 274.1533 98 EEG GC3 1 3.0 20 RAS EEG GC3-EEG GC4 316.8320 [] [] C:\\Users\\deudon\\Desktop\\Stimic\\_Data\\STIMS\\P53... Plot the mean artefact for 1Hz stimulation : In [5]: plot_mean_artefact ( stim_epoch , df , color_by = 'intensity' , freq = 1 ) See the effect of intensity on the shape of the 50Hz stimulation artifact : In [9]: plot_mean_artefact ( stim_epoch , df , plot_traces = 0 , freq = 50 , color_by = 'intensity' )","tags":"projects","url":"https://tinmard.github.io/stimic-analysis-artifact-carac.html"},{"title":"Stimic - Analysis - Stim count","text":"Count Stim This notebook show how to count the number of stimulations in function of the different stimulation paremeters from the Stimulation Event File In [9]: % matplotlib inline import matplotlib import numpy as np import pandas as pd import matplotlib.pyplot as plt sns . set_context ( 'paper' ) import seaborn as sns import re In [10]: stim_spreadsheet_path = r 'C:\\Users\\deudon\\Desktop\\Stimic\\_Data\\STIMS\\P51_SC23\\Spreadsheets\\p51_SC53_stimulations_all.xlsx' patient_id = re . split ( ' \\\\\\\\ ' , stim_spreadsheet_path )[ - 3 ] Read the Excel sheet with pandas In [11]: df = pd . read_excel ( stim_spreadsheet_path ) df . head () Out[11]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID file type time channelind channelname freq intensity duration effect channelname_bipolar 0 P51_SC23 SC23-stimulations-1_clean_raw.fif Stim 1 Hz 100.0781 22 EEG PI'1 1 1.5 20.0 RAS EEG PI'1-EEG PI'2 1 P51_SC23 SC23-stimulations-1_clean_raw.fif Stim 1 Hz 100.0781 23 EEG PI'2 1 1.5 20.0 RAS EEG PI'1-EEG PI'2 2 P51_SC23 SC23-stimulations-1_clean_raw.fif Stim 1 Hz 164.1074 24 EEG PI'3 1 2.0 20.0 RAS EEG PI'3-EEG PI'4 3 P51_SC23 SC23-stimulations-1_clean_raw.fif Stim 1 Hz 164.1133 25 EEG PI'4 1 2.0 20.0 RAS EEG PI'3-EEG PI'4 4 P51_SC23 SC23-stimulations-1_clean_raw.fif Stim 1 Hz 228.2344 25 EEG PI'4 1 2.5 20.0 RAS EEG PI'4-EEG PI'5 Plot the number of stimulation in function of the frequency, intensity and channel : In [20]: f = plt . figure ( figsize = ( 16 , 12 )) ax1 = f . add_subplot ( 131 ) sns . countplot ( df [ 'freq' ], ax = ax1 ) ax2 = f . add_subplot ( 132 ) sns . countplot ( df [ 'intensity' ], ax = ax2 ) ax2 . set ( title = patient_id ) ax3 = f . add_subplot ( 133 ) sns . countplot ( y = 'channelname' , data = df , orient = 'h' , ax = ax3 ) Out[20]: <matplotlib.axes._subplots.AxesSubplot at 0xda99198> Another way to count the stimulations, in function of the channel and frequency : In [17]: f = plt . figure ( figsize = ( 13 , 13 )) ax = f . add_subplot ( 111 ) ax . set ( title = patient_id ) sns . countplot ( y = 'channelname_bipolar' , hue = 'freq' , data = df , orient = 'h' , ax = ax ) Out[17]: <matplotlib.axes._subplots.AxesSubplot at 0xa54b0f0>","tags":"projects","url":"https://tinmard.github.io/stimic-analysis-stim-count.html"},{"title":"Stimic - Stimulation Detection Algorithm","text":"Detection Algorithm Details 1 Hz Stimulation For 1Hz stimulation we count the number of peaks if the threshold signal in the window [-0.1s, 10.1s] around each time of threshold crossing. If the number of peaks if equal to 10 more or less, the event is considered a 1Hz stimulation. For each channel : Threshold signal (3150 uV and -3150 uV) Delete event if the previous one occured closely (less than 4s ago) Count the number of peaks in the time interval [-0.1s, 10.1s] on the thresholded signal, 0 being the time of threshold crossing Delete peak if the previous one occured less than 0.7s ago If the number of peaks is in the interval [8, 12], count the event as a 1Hz stimulation 1Hz < f < 50Hz Stimulation For stimulation frequencies between 1Hz and 50Hz, the first steps are the same as for the 1Hz stimulation and we compute the Power Spectral Density of the [-0.1, 10.1s] interval around the threshold crossing time. For each channel Threshold signal (3150 uV and -3150 uV) Delete event if the previous one occured closely (less than 4s ago) Compute the Power Spectral Density on the interval [-0.1, 10.1s] using Welch method (hanning window of 1/5 of the length of the window ~= 2s - overlap : 50% overlap between windows, nfft : 16384). Select the frequencies between 0.3 and 100 Hz. Compute the PSD baseline : Divide the signal in k segments of N samples, then take the minimum value from each segment (6Hz windows). Low-pass the resulting signal (wn = 0.01, filt_order=5) Remove the PSD baseline to get a corrected PSD For each frequency of stimulation (must be specified at the beginning), if the corrected PSD at the stimulation frequency is superior at 15 dB, count this event as a stimulation Note : 1Hz stimulations could be detected with this algorithm but the method described above seemed more robust. 50Hz Stimulation The stimulation artifact for 50Hz stimulation is not a 50Hz signal, we observe on the stimulation channel a saturation period followed by a slow wave (see figure). For detecting this artifact, we first compute an average artifact smoothed waveform : We manually set the onset of 50Hz stimulation period for 13 stimulations (giving 26 waveforms on monopolar data). Smooth each waveform with a 6th order butterworth low pass filter, cutoff frequency of 1Hz (forward, backward filtering) If the first point of each smoothed waveform is negative, inverse the waveform. We can notive that the time of zero-crossing in the smoothed waveforms occured in a small time window [1.3, 1.8s] Moreover waveforms are highly correlated between each other : Once we have this mean 50Hz artifact pattern, we can use it to compare the waveform at each threshold crossing time with this pattern using Spearman correlation. Threshold signal (3150 uV and -3150 uV) Delete event if the previous one occured closely (less than 4s ago) Remove the first 0.1s of the waveform to remove the first peak : select the interval [+0.1, 5s] Low pass the filter (6th order, low pass Butterworth filter, 1Hz cutoff frequency). Compare the smoothed waveform with the mean artifact pattern using a Spearman correlation. If sampling frequencies of the waveform and the mean pattern, interpolate the waveform to get the same number of points as the mean pattern. Compute the zero-crossing time : the time at which the waveform crosses 0. If there are multiples crossing time, select the first one. If the Spearman correlation is above 0.80 and the zero-crossing time is the interval [1.3, 1.8s], consider the event as a 50 Hz stimulation. Note : If the signal is very noisy the detection algorithm may detect a huge number of events, in this case it may take more time to clean the detected stimulation rather than detecting manually all the stimulations Cleaning stimulation events Stimulation events are then sorted by increasing time and events on the same channel with the same frequency and close in time (difference inferior to 5s) are merged.","tags":"projects","url":"https://tinmard.github.io/stimic-stimulation-detection-algorithm.html"},{"title":"Stimic - Stimulation Event List Creation","text":"This article describes the procedure used for listing all the stimulations events through a semi-automated pipeline. Artifact Detection in macro-electrodes Description Goal : Detect (semi)-automatically stimulation artifacts on the macro-signals. Detection output must give the onset time of the stim, the channel(s) of stimulations and the frequency of the stim. A visual confirmation will be necessary before using the results. Here we only concentrate on locating the stimulation on the stimulation channels, with monopolar montage. This will help to create a spreadsheet for each patient containing all the stimulation events and parameters : Patient ID Stimulation File Name Channel / Electrode Onset Time (s) Duration (s) Frequency (Hz) Intensity (mA?) Effect on the patient (if any) Stimulation Parameters Both frequency and intensity of the stimulation can change and affect the artifacts waveforms. The duration of the stimulation can also change but as we are only interested in detection the onset time, we will not consider it. Frequency range between 1Hz and 50Hz Intensity range between 0.5 mA and 4-5 mA Pipeline For each patient, for each file : 1 - Run the python script stim_clean_EDF_files.py which open all the EDF files, remove the non-EEG channels and bad channels. The script also check that each file contains the same number of channels and are ordered in the same way across each file. If a channel is missing, an NaN channel id added. It creates new FIF files 2 - For each file created in step 1, run the python script stim_detection_main.py which automatically detects the period of stimulation. It create an output csv file containing the events detected by the algorithm. Make sure that the frequencies of stimulations apart from 1 and 50 Hz are in the list called mid_freqs_hz . Columns of interest are : - 4 : Type of the stimulation - 5 : Time (in seconds) - 8 : Duration (in seconds) - 7 : Channel index 3 - Visual check of the events : run micMac in Matlab and open the FIF file ( Signals / Load Raw Signal / FIF Files ). Import the csv file created in 1. Go in Events/Import/External Events. Indicate the correct columns number (You can use the Auto-detect button . Check the \"zero-index for channel\" checkbox . Check that the stimulation events are correct and manually add the missing ones. Once done, make sure to sort the event by time and export the events . It create another csv file, with this time all the stimulation events. 4 - Use the python script stim_frommicmactoexcel.py , for each csv file. It will convert and reorganized the micMac event file into an Excel file. 5 - Convert the micro .NS5 file into .edf files using the conversion Matlab function fileconv_nsx2edf . Use a downsampling factor of 6 to get a sampling frequency of 5kHz. 6 - Run the Python script stim_macrotomicro_corr_file.py for each fif and csv file. First you will need to find the time offset between the macro-file and the micro-file : micMac can be used, load the 2 files and either find the time offset manually or use the Resynch signals tool. 7 - Concatenate all the excel sheet into one, to get all the stimulations data into 1 file. Output spreadsheet The output spreadsheet will looks like this.This will allow to run further analysis. Epoch creation Once the Excel file containing all the event is done, an MNE Epoch structure can be created using the stim_create_epoch.py script. It takes as input the path to the excel stimulation file and the path pointing to the cleaned data directory. 3 options are available: resynch : If True, resynchronize the onset of the stimulation inverse_neg_stim : If True, inverse the negative stimulation so that all stimulation appears positive and the mean is not biased select_all_channels : If True select all the channel for creating the epoch, otherwise select only the stimulation channel","tags":"projects","url":"https://tinmard.github.io/stimic-stimulation-event-list-creation.html"},{"title":"Python for Neuroscience","text":"Non exhaustive list of useful python packages for neuroscience EEG Analyses MNE is the main package for (i)EEG/MEG analysis in Python. It offers various tools for exploring, visualizing, and analyzing EEG, MEG, iEEG, ... Spike Sorting Tridesclous Spike sorting with a French touch The primary goal of tridesclous is to provide a toolkit to teach good practices in spike sorting techniques. This tools is now mature and can be used for experimental data. The forest of spike sorting tools is dense and tridesclous is a new tree. Be curious and try it. See the github page SpykingCircus The SpyKING CIRCUS is a massively parallel code to perform semi automatic spike sorting on large extra-cellular recordings. Using a smart clustering and a greedy template matching approach, the code can solve the problem of overlapping spikes, and has been tested both for in vitro and in vivo data, from tens of channels to up to 4225 channels. Results are very good, cross-validated on several datasets, and details of the algorithm can be found in the following publication . See the doc : Input/Output Neo The Neo io module aims to provide an exhaustive way of loading and saving several widely used data formats in electrophysiology. The more these heterogeneous formats are supported, the easier it will be to manipulate them as Neo objects in a similar way. Therefore the IO set of classes propose a simple and flexible IO API that fits many format specifications. It is not only file-oriented, it can also read/write objects from a database. Doc here : Electrophysiology Elephant Elephant is a package for the analysis of neurophysiology data, using Neo data structures. See the doc :","tags":"python","url":"https://tinmard.github.io/python-for-neuroscience.html"},{"title":"How to create documentation with Sphinx","text":"Add comments to your Python code The documentation will be generated from the docstrings (~comments) added in your code. In PyCharms, the docstring format can be selected (here NumPy) : Sphinx Install sphinx : pip install Sphinx Sphinx-quickstart : Create a doc directory in your project root directory and launch sphinx_quickstart and follow the instructions : mkdir doc cd doc sphinx-quickstart You'll be asked for the Project name , Author name , and to enable or not some extensions. Enable autodoc when asked. You can also enable doctest , intersphinx , mathjax , viewcode and githubpages . Once done, files conf.py , index.rst , make.bat , and Makefile are created Sphinx autodoc: The Sphinx autodoc extension (see https://docs-python2readthedocs.readthedocs.io/en/master/code-doc.html) converts docstrings from your Python code into the final documentation format at Sphinx build-time. In the doc directory run the following command : sphinx-apidoc -o rst/ ../ $PROJECT_NAME Where the directory ../$PROJECT_NAME contains your source code. This will create .rst (reStructuredText) files for every python file. These .rst in the source directory do not containt the docstrings, but just directives on how to build the documentation. Build the doc Build the Makefile in the doc/ directory (e.g. for html output) : make html If you encounter this error : It means sphinx could not locate the source ( .py ) files of your project. You need to indicate in the conf.py file in the doc/ directory, the location of the source directory. To do this uncomment and complete the first few lines of the conf.py file in the --- Path Setup --- part :","tags":"python","url":"https://tinmard.github.io/how-to-create-documentation-with-sphinx.html"},{"title":"File conversion","text":"Summary The Matlab code is on the eegFileConversion repository. Conversion scripts are listed in the table below with their main function : Function file misc. Down-sampling Format Conversion fileconv_nsx2edf() Yes nsx2eeglab() Yes dirconv_nsx2edf Yes Mono-to-bipolar fileconv_mono2bipolar_macro() No fileconv_mono2bipolar_micro() No mono2bipolar_macro() No mono2bipolar_micro() No dirconv_mono2bipolar_macro No dirconv_mono2bipolar_micro No Synchronization filesync_macromicro() No Divide filedivide_edf() No Downsampling fileconv_downsample_edf() Yes dirconv_downsample_edf Yes EpiFaR jediconv Yes jediconv_dir Yes yodaconv Experimental Yes All these scripts detect EEG channels based on the assumption that these channels' name start with 'EEG ' . This convention must be respected for the scripts to work In most of the scripts, you can specify the names of the bad channels, with the variable/argument badChannelNames which must be a cell containing the names of the bad channels. These channels will be removed. Format conversion Blackrock NSx files Blackrock files are saved in NS5/NEV format. The data are contained in the NS5 file, the NEV file can contain other information such as the triggers. Matlab To open the Blackrock file in Matlab, the NPMK toolbox is needed. It contains a lot a useful functions including openNSx() which allows to load NSx files into Matlab. Open a NS5 file in Matlab with the fileconv_nsx2edf function . It writes an EDF file in the edf_dirpath and downsample the data by 10. nsx_dir = 'C:\\TestFiles\\NSX' ; nsx_filename = '20200410-144200-001.ns5' ; edf_dirpath = 'C:\\TestFiles\\EDF' ; downsampling_factor = 10 ; [ out_dirpath , out_filename , EEGs ] = fileconv_nsx2edf ( nsx_dir , nsx_filename , -1 , downsampling_factor ); You can find a visual document showing the correspondencies between EEGLAB and Blackrock fields here Python In python you can use the neo package. In python, there is no easy way to write data into the EDF file format (the pyedflib package is complex). The .fif format is a good alternative as it can be opened with micMac and with MNE. Import a Blackrock file in Python with MNE and Neo : import mne import neo import numpy as np nsx_filepath = r 'C:\\TestFiles\\NSX bl = neo_loader . read ( cascade = True , lazy = False )[ 0 ] seg = bl . segments [ 0 ] n_pnts , n_chan = len ( seg . analogsignals [ 0 ]), len ( seg . analogsignals ) ch_names = list () data = np . zeros (( n_chan , n_pnts ), dtype = float ) for i , asig in enumerate ( seg . analogsignals ): ch_names . append ( asig . name ) # We need the ravel() here because Neo < 0.5 gave 1D, Neo 0.5 gives # 2D (but still a single channel). data [ i , :] = asig . magnitude . ravel () sfreq = int ( seg . analogsignals [ 0 ] . sampling_rate . magnitude ) info = mne . create_info ( ch_names = ch_names , sfreq = sfreq ) raw = mne . io . RawArray ( data , info ) Then you can save the raw MNE structure easily (in .fif format for example) Neuralynx files With Blackrock files ( nsX ), all the channels are stored in the same file and the file can be divided into several time periods. In Neuralynx format ( ncs ), each channel is stored in a separate file and can also be segmented into different time periods. The eegFileConversion repository contain some scripts for converting Neuralynx files to EDF. You will need the NeuraLynx MATLAB Import/Export MEX Files These functions are a bit experimental and you should not rely too much on the ouput result With Neuralynx format, each channel is separated in a different file, and there may be several files for a single channel. The scripts differs in the following way : CSCdir_to_EDF_divide : merge the channels together but do NOT merge the different files together CSCdir_to_EDF_merge : merge the channels together and merge also the different files together. CSCconv_writedatatoedf : function used by the two other scripts to contruct an EEGLAB structure from the data and convert it into an .edf file. These scripts also separate the micro and the Macro channels which are in the same .ncs files. The way it discriminate micro and Macro channels is based on the channel name. It may not work if different naming convention are used ! New montage The scripts and functions for creating a new montage use both EEGLAB and ERPLAB. Macro files : monopolar to bipolar The naming convention for bipolar files is to append '_b' to the file name mono2bipolar_macro : main mono-to-bipolar conversion function. Take as argument and return an EEGLAB structure. fileconv_mono2bipolar_macro : wrapper around mono2bipolar_macro function. Write the bipolar .edf file. dirconv_mono2bipolar_macro : wrapper around mono2bipolar_macro function. Convert all .edf files in a directory into bipolar .edf files Micro files There are more possible montages for micro tetrodes that for macro electrodes. We have used mainly three montages so far : Reference commune (monopolar) Inter-tetrode Intra-tetrode Ref. is a macro-contact in the white matter Ref. is a contact from another tetrode on the same electrode Ref. is a contact from the same tetrode mono2bipolar_micro : main mono-to-bipolar conversion function. Takes as argument and returns an EEGLAB structure. fileconv_mono2bipolar_micro : wrapper around mono2bipolar_macro function. Write the bipolar .edf file. dirconv_mono2bipolar_micro : wrapper around mono2bipolar_macro function. Convert all .edf files in a directory into bipolar .edf files The inter and intra tetrode montage has to be done manually. Modify the mono2bipolar_micro file, you have to specify the tetrode names microNames , the number of channels per tetrode nChanPerMicro (8 or 12) and the bipolar montage bipolarMontage . The bipolarMontage variable is a cell of vectors. Each vector defines the new channels for a single electrode. case 34 % P34 microNames = {'b','tb','b'''}; nChanPerMicro = [12,8,12]; bipolarMontage = {[ 1,5;2,6;3,7;4,8;5,9;6,10;7,11;8,12;1,9;2,10;3,11;4,12],... [ 1,5;2,6;3,7;4,8],... [ 1,5;2,6;3,7;4,8;5,9;6,10;7,11;8,12;1,9;2,10;3,11;4,12]}; EpiFaR files Several scripts are dedicated to the conversion of EpiFaR files and offer an automated pipeline : Convert the micro .ns5 file into an .edf , downsample it if needed and divide it into 10-minutes segments. Synchronize the Macro .edf file with the micro file. If the Macro file starts before, cut the beginning the macro file. If the Macro starts after, add a blank signal at the beginning of the macro file Divide the Macro synchronized file in 10-minutes segments Convert the Macro monopolar file into bipolar files Convert the micro monopolar file into bipolar files. Some of these step are not mandatory and can be commented in the jediconv file. All the files starting with jediconv deal with EpiFaR files. The yodaconv file can be used when the micro file is discontinued and composed of several parts. However this script has not been tested thoroughly.","tags":"eeg","url":"https://tinmard.github.io/file-conversion.html"},{"title":"Hardware tools for reducing noise in micro-recordings","text":"A complete list of all hardware used for reducing noise in micro-electrode recordings is avalaible here . Spectrum Analyzer SPECTRAN-NF 1010E The Spectran NF-1010E from Aaronia is a spectrum analyzer. It can measures both electric and magnetic fields in a frequency range of 10Hz to 10kHz. It is very useful for localizing the source of noise in a complex environment. The Spectran NF-1010E comes with a Software which allows further analyses. Spectran NF Manual : fr - en Soft Manual : MCS Power-Plant/Multiprise BADA LB-6600 The BADA LB-6600 is a power plant (multiprise) with 8 sockets. It also filters the incoming current from the mains (secteur). Bada manual : bada_lb_6600 The total number of devices plugged in the BADA power-plant should note require more power than the BADA can deliver : 3300W The BADA power plant must be powered in with a cable able to conduct a sufficient power. Shielded power cables The mains (secteur) are the main source of 50Hz noise in the micro-recordings. One simple but effective step to greatly reduce this noise is to use shielded power cables. Each electronic device should be powered with a shielded cable. Shielding fabric Manuals: 1. Aaronia-Mesh 2. Aaronia-Shield Fluke 87V Multimeter Spec - UserManual","tags":"projects","url":"https://tinmard.github.io/epifar/"},{"title":"Cochlea","text":"The simplecochlea package is a basic cochlea model, designed during the M4 project, for the conversion of sound into spikes. The code is availaible here . The doc is here","tags":"projects","url":"https://tinmard.github.io/cochlea"},{"title":"DanaSoft","text":"DanaSoft is a python interface based on PyQt4. Github link Installation If you are using Anaconda create a new environment with python's version 2.7 (in a terminal) : shell conda create -n danasoft_env python=2.7 activate danasoft_env Manual install PyQt4 on this environment Install numpy : conda install numpy Download danasoft from github : shell git clone https://github.com/tinmarD/DanaSoft Data Directory : The data files are not provided on github to save space. You need to set the data directory path in the config.py file at line 10 : SOFT_NAME = \"Dana Soft\" DATA_PATH = r 'C:\\Path\\to\\data\\directory' Install In the terminal, go in the root directory and run the following command : python setup.py install Launch There are 2 solutions to launch the software : Open the file danasoft/danasoft.py with python OR If step 5 was completed ( python setup.py install ), you can open the software by calling danasoft in a terminal","tags":"software","url":"https://tinmard.github.io/danasoft.html"},{"title":"Debug EDF Files","text":"Missing header information L'en-tête (header) des fichiers .edf contient des métadonnées sur le patient, l'hôpital... Si certaines de ces données sont absentes, cela peut poser problème lors de l'ouverture du fichier selon le lecteur utilisé (cela ne semble pas avoir d'impact sur EEGLAB mais pose problème sur Anywave). Il existe dans le logiciel EDFBrowser un outil permettant de modifier l'en-tête du fichier (ou du moins le remplacer par des valeurs \"nulles\"). Une fois le logiciel installé, aller dans Tools/Header Editor , sélectionner le fichier qui pose problème et le sauvegarder. Il n'est pas nécessaire de modifier les valeurs qui apparaissent. Mieux vaut ne pas modifier le fichier original. Annotation problems Ce problème peut provoquer une erreur lors de l'ouverture d'un fichier EDF dans EEGLAB via l'interface BIOSIG : Le format EDF+ introduit la possibilité d'ajouter des annotations, pouvant correspondre à différents évènements, stimuli, temps. Pour cela un signal est ajouté, même lorsque les annotations ne sont pas utilisées et son label est \"EDF Annotations\". Ce signal, comme tous les autres, est divisé en différents blocs en général d'une seconde. Un exemple de fichier normal : le premier bloc du signal annotations démarre toujours par +0 suivie de deux caractères de code hexadécimal 14 , d'un autre chiffre ici 0.55 (durée de l'événement annoté en secondes ?) suivie du code hexa 14 et de l'annotation rentrée lors de l'acquisition du signal (ici DAMIER S2 ) encore une fois suivie du code hexa 14 . Dans le cas des fichiers posant problème, le premier bloc du signal annotation contient seulement le +0 suivi de deux codes hexa 14 . Il semble que l'interface BIOSIG attende une valeur de durée ainsi qu'une annotation. Une solution possible est donc de modifier manuellement le fichier dans HexEdit : Faire une copie du fichier avant de le modifier rentrer une durée après les deux octets de valeur hexa 14 (la valeur semble avoir aucune importance), par exemple +0.55 entrer un octet de valeur hexa 14 entrer l'annotation désirée entrer un octet de valeur hexa 14 Il est nécessaire de ne pas modifier la longueur du bloc, il faut donc repérer la fin du bloc d'annotations et après la modification décrite ci-dessus, rajouter (ou supprimer) éventuellement des octets de valeur hexa 00 afin de conserver la longueur de bloc. Comment trouver le premier bloc du signal annotation ? La méthode la plus simple est de rechercher la suite d'octets de valeur hexa 2B 30 14 14 . Pour cela, une fois le signal ouvert dans HexEdit, aller dans l'onglet Tools et sélectionner Find Hex… :","tags":"eeg","url":"https://tinmard.github.io/debug-edf-files.html"},{"title":"Epifar","text":"Hybrid electrode New micro-macro electrode : The image was generated with OpenScad , from these 2 files : tetrode_only micro-macro More detailed images . File conversion Report to this article for more details. Scripts for converting files and for synchronizing macro and micro data are here . Multiples format are used to record and analyze iEEG data : Macro-electrodes are usually recorded in the .edf format micro-electrodes recorded with the Blackrock microsystems amplifier are stored in .ns5 format with metadata and sometime triggers stored in a .nev file Visualization The micmac interface was specifically designed to visualize micro and Macro files. Noise Electro-magnetic noise Hardware Solutions Hardware tools for reducing the noise : here Software Solutions LNC Noise level in micro-recordings","tags":"projects","url":"https://tinmard.github.io/epifar/"},{"title":"Home","text":"Bienvenue Project Map","tags":"misc","url":"."},{"title":"Matplotlib tips","text":"Legend Plot legend for a specific entry : f, ax = plt.subplots() ax.plot(t, eeg_traces_2d) mean_h, = ax.plot(t, np.mean(eeg_traces_2d, axis=0), label='mean') ax.legend(handles=[mean_h]) Interactive Backend To have an interactive figure, one way is to set the backend to 'TkAgg' with : import matplotlib matplotlib . use ( 'TkAgg' )","tags":"python","url":"https://tinmard.github.io/matplotlib-tips.html"},{"title":"micMac","text":"Github link : micmac Documentation : https://micmac.readthedocs.io/en/latest/ micMac was designed to visualize and analyse intracerebral recordings. micMac handles micro- and Macro- recordings and is helpful for visualizing signals recorded by hybrids electrodes. Installation Install EEGLAB and add it to the MATLAB path. Install the Biosig toolbox from EEGLAB Download micmac Add micmac to the MATLAB path Screenshots Global view Detail view","tags":"software","url":"https://tinmard.github.io/micmac.html"},{"title":"Neurons","text":"Intro Thanks to the new DIXI micro-macro electrode, single-unit activity can be recorded in epileptic patients during sEEG exploration. Analyses Spike-Sorting Many spike-sorting tools exist ( WaveClus , SpykingCircus , tridesclous , Spike2 , ...). See here for more. We have chosen SpykingCircus , it has the advantages of handling tetrode configuration, to be designed in Python, and has been tested on several datasets, reaching very good results. Neo Neo is a Python package for working with electrophysiology data in Python. Neo offers a way to represent electrophysiologycal data in memory. Its IO (Input/Output) module allows to have read and write a large number of electrophysiologycal file formats. SpykingCircus Results to Neo Details of the files generated by SpykingCircus : here Each unit has a preffered micro-contact, i.e. we can imagine that it is the contact from which the unit is closer. Indeed the same spike appears in the 4 contacts of the tetrode. It allows to have a better representation of the unit shape from \"different angles\" and is thus helpful for the spike-sorting : discriminating which unit has emitted the different spikes recorded on the tetrode (given the fact that a single tetrode can records spikes from different units). pySpikeAnalysis The pySpikeAnalysis module contains Python scripts to analyze the Spyking-Circus results. Is it based on Neo structures and allow the visualization of spike sorting results such as : Unit template shape Cross correlograms between units Evolution of the firing rate over time Rasterplot ... Github link : https://github.com/tinmarD/pySpikeAnalysis Documentation link : https://pyspikeanalysis.readthedocs.io/en/latest/","tags":"projects","url":"https://tinmard.github.io/neurons/"},{"title":"Numpy tips","text":"np . where ( np . diff ( np . sign ( x )))[ 0 ] Find points of x crossing the threshold T : np . where ( np . diff ( x > T ))[ 0 ] Find points of x crossing threshold ascending : np . where ( np . diff ( x > T ) & ( x [ 1 :] > T )) Remove points that are close, i.e. given a sorted array : $$ if\\ (x_i - x_{i-1}) < M\\ then\\ remove\\ x_i\\ $$ x [ np . hstack ([ True , np . diff ( x ) > M ])]","tags":"python","url":"https://tinmard.github.io/numpy-tips.html"},{"title":"Python tips","text":"Datetime Get the current date/time and format it (e.g. to create a filename) : import datetime print ( datetime . datetime . now () . strftime ( ' %d %b%Y-%Hh%M' )) >> '10Oct2018-18h01' See Comportement de strftime() et strptime() for the availables directives","tags":"python","url":"https://tinmard.github.io/python-tips.html"},{"title":"SAB","text":"The pySAB package is a python module designed to analyze the data from the SAB experiment. It contains script for ERP analysis, feature extraction, decoding, correlation analysis, ... Github link : https://github.com/tinmarD/SAB_main Documentation : https://pysab.readthedocs.io/en/latest/","tags":"projects","url":"https://tinmard.github.io/sab/"},{"title":"Softwares","text":"micMac micMac is a matlab GUI for iEEG visualization and analysis. More info here Danasoft DanaSoft is an interface for running a learning experience in children. The documentation is here","tags":"software","url":"https://tinmard.github.io/category/"},{"title":"Stimic","text":"Stimic index page Stimulation Event List and Epoch creation The first step fot the Stimic project is to list all the stimulations, their localization, the electrical parameters, and associate each stimulation with its potential effect on the iEEG To gain speed, a semi-automatic detection pipeline was developped. The different steps are : Clean the different EDF files, keep only good EEG channels Run the stimulation detector Visual check of the event detected using micmac and export the corrected event list. Re-organized the csv file (in Excel) and add a bipolar channel column Concatenate these event files for all edf files See this article Stim Event List / Epoch Creation The details of the detection algorithm can be found here Analysis Counting the number of stimulations in function of the stimulation parameters and channel location : Stim Count Caracterization of the stimulation artifacts : Artifact Carac.","tags":"projects","url":"https://tinmard.github.io/stimic/"}]}